{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Register\n",
    "the xxEnv, env_cfg and agent_cfg is how to convey in this framework\n",
    "\n",
    "- 首先要有个概念：我们的env_cfg继承了DirectRLEnvCfg，agent_cfg继承了RslRlOnPolicyRunnerCfg，xxEnv继承了DirectRLEnv，也就继承了一些方法（函数）和一些默认属性property。当然也有些方法只有声明，没有实现implement，例如the '_get_observations' method就是在我们的xxEnv中具体实现的。\n",
    "- 其次，通过gym.register(id=\"xxtask\", entry_point=\"xxEnv\", kwargs={}, )注册了env。其中xxEnv是继承了DirectRLEnv的类，这个类中定义了env的属性和函数。\n",
    "\n",
    "- 然后，由gym.make(id, cfg=env_cfg, )创建的env，又通过RslRlVecEnvWrapper()进行了wrap\n",
    "- 最后传入了OnPolicyRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts/rsl_rl/train.py\n",
    "```python\n",
    "@hydra_task_config(args_cli.task, \"rsl_rl_cfg_entry_point\")\n",
    "def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\n",
    "    # ...\n",
    "    # create isaac environment\n",
    "    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\n",
    "    # ...\n",
    "    # wrap around environment for rsl-rl\n",
    "    env = RslRlVecEnvWrapper(env)\n",
    "    # create runner from rsl-rl\n",
    "    runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)\n",
    "    # ...\n",
    "    # run training\n",
    "    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)\n",
    "    # close the simulator\n",
    "    env.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 gym.register()\n",
    "Gym 使用一个注册表（registry）来管理所有可用的环境。每个环境都有一个唯一的 ID，并且在注册时指定了如何实例化该环境: `entry_point`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常，环境通过 `gym.register` 函数进行注册。\n",
    "```python\n",
    "\"\"\"Registers an environment in gymnasium with an ``id`` to use with :meth:`gymnasium.make` with the ``entry_point`` being a string or callable for creating the environment.\n",
    "It takes arbitrary keyword arguments, which are passed to the :class:`EnvSpec` ``kwargs`` parameter.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".e.g例如\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from . import agents\n",
    "from .zbot6b_env_v0 import ZbotBEnv, ZbotBEnvCfg\n",
    "##\n",
    "# Register Gym environments.\n",
    "##\n",
    "gym.register(\n",
    "    id=\"Zbot-6b-walking-v0\",\n",
    "    entry_point=\"Zbot.tasks.moving.zbot6b_direct:ZbotBEnv\",\n",
    "    disable_env_checker=True,\n",
    "    kwargs={\n",
    "        \"env_cfg_entry_point\": ZbotBEnvCfg, \n",
    "        \"rsl_rl_cfg_entry_point\": f\"{agents.__name__}.rsl_rl_ppo_cfg:ZbotSBFlatPPORunnerCfg\",\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 env_cfg & agent_cfg\n",
    "decorator 负责解析并提供 env_cfg 和 agent_cfg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts/rsl_rl/train.py\n",
    "```python\n",
    "@hydra_task_config(args_cli.task, \"rsl_rl_cfg_entry_point\")\n",
    "def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def hydra_task_config(task_name: str, agent_cfg_entry_point: str) -> Callable:\n",
    "    #...\n",
    "    env_cfg, agent_cfg = register_task_to_hydra(task_name, agent_cfg_entry_point)\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def register_task_to_hydra(\n",
    "    task_name: str, agent_cfg_entry_point: str\n",
    ") -> tuple[ManagerBasedRLEnvCfg | DirectRLEnvCfg, dict]：\n",
    "    # ...\n",
    "    env_cfg = load_cfg_from_registry(task_name, \"env_cfg_entry_point\")\n",
    "    agent_cfg = load_cfg_from_registry(task_name, agent_cfg_entry_point)\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def load_cfg_from_registry(task_name: str, entry_point_key: str) -> dict | object:\n",
    "    \"\"\"It supports both YAML and Python configuration files.\n",
    "    If the entry point is a YAML file, it is parsed into a dictionary.\n",
    "    If the entry point is a Python class, it is instantiated and returned.\"\"\"\n",
    "    # obtain the configuration entry point\n",
    "    cfg_entry_point = gym.spec(task_name).kwargs.get(entry_point_key)\n",
    "    # 如果 cfg_entry_point 是一个以 .yaml 结尾的字符串\n",
    "    # 如果 cfg_entry_point 是可调用的（例如是一个函数或类）\n",
    "    # 如果 cfg_entry_point 是字符串（格式为 \"module_name:attr_name\"）\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 gym.make()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gym.make` 是 Gym 库中的一个核心函数，用于创建和初始化环境实例。它根据给定的环境 ID 和其他参数来实例化相应的环境类。\n",
    "```python\n",
    "import gymnasium as gym\n",
    "gym.register(\n",
    "    id='CustomEnv-v0',\n",
    "    entry_point='custom_env_module:CustomEnv',\n",
    "    kwargs={\n",
    "        \"env_cfg_entry_point\": xxEnvCfg, \n",
    "        \"rsl_rl_cfg_entry_point\": \"xxAgentCfg\",\n",
    "    }\n",
    ")\n",
    "```\n",
    "```python\n",
    "env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\n",
    "```\n",
    "- 解析环境 ID\n",
    "\n",
    "当调用 `gym.make` 时，Gym 会解析传入的 `args_cli.task`，并从注册表中查找对应id的环境条目。如果找到了匹配的条目，则使用其 `entry_point` 来实例化环境。\n",
    "- 实例化环境\n",
    "\n",
    "`gym.make` 会根据找到的 `entry_point` 动态导入模块并实例化环境类。例如，如果 `entry_point` 是 `'custom_env_module:CustomEnv'`，那么 Gym 会导入 `custom_env_module` 模块并调用 `CustomEnv` 构造函数来创建环境实例。\n",
    "- 传递参数\n",
    "\n",
    "除了环境 ID，`gym.make` 还可以接受额外的关键字参数，这些参数会传递给环境构造函数。例如：`cfg=env_cfg` 和 `render_mode=\"rgb_array\" if args_cli.video else None` 是传递给 `CustomEnv` 构造函数的额外参数。\n",
    "> 注意：`env_cfg`又是在# 0.2 中通过解析注册表参数得到的。\n",
    "> \n",
    "> `gym.spec(task_name).kwargs.get(\"env_cfg_entry_point\")`\n",
    "- 总结\n",
    "\n",
    "`gym.make` 通过注册表查找环境 ID，动态导入并实例化相应的环境类，并将额外的参数传递给环境构造函数。最终返回一个初始化好的环境实例，供用户使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gymnasium/envs/registration.py\n",
    "```python\n",
    "env_creator = env_spec.entry_point | env_creator = _load_env_creator(env_spec.entry_point)\n",
    "# env_creator = custom_env_module.CustomEnv .e.g \n",
    "env = env_creator(**env_spec_kwargs)  # 使用解包后的 env_spec_kwargs 创建环境实例\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 *args & **kwargs\n",
    "在 Python 中，函数或方法的参数可以分为以下几类：\n",
    "- **位置参数（positional arguments）**：必须按顺序传递，不能省略（除非有默认值）。\n",
    "- **关键字参数（keyword arguments）**：通过参数名传递，顺序无关紧要。\n",
    "- **默认参数（default arguments）**：提供默认值，使某些参数成为可选参数。\n",
    "- **可变长度参数（variadic arguments）**：`*args` 和 `**kwargs` 提供了极大的灵活性，允许函数接受任意数量的位置参数和关键字参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 位置参数（Positional Arguments）\n",
    "- **定义**：必须按照定义时的顺序传递给函数。\n",
    "- **特点**：不能省略，除非有默认值。\n",
    "- **示例**：\n",
    "```python\n",
    "def greet(name, greeting):\n",
    "    print(f\"{greeting}, {name}!\")\n",
    "\n",
    "greet(\"Alice\", \"Hello\")  # 输出: Hello, Alice!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 关键字参数（Keyword Arguments）\n",
    "- **定义**：通过参数名传递给函数，顺序无关紧要。\n",
    "- **特点**：可以提高代码的可读性，避免混淆参数的顺序。\n",
    "- **示例**：\n",
    "```python\n",
    "def greet(name, greeting):\n",
    "    print(f\"{greeting}, {name}!\")\n",
    "\n",
    "greet(greeting=\"Hi\", name=\"Bob\")  # 输出: Hi, Bob!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 默认参数（Default Arguments）\n",
    "- **定义**：为参数提供默认值，如果调用时未提供该参数，则使用默认值。\n",
    "- **特点**：使某些参数成为可选参数。\n",
    "- **示例**：\n",
    "```python\n",
    "def greet(name, greeting=\"Hello\"):\n",
    "    print(f\"{greeting}, {name}!\")\n",
    "\n",
    "greet(\"Alice\")  # 输出: Hello, Alice!\n",
    "greet(\"Bob\", \"Hi\")  # 输出: Hi, Bob!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 可变长度参数（Variadic Arguments）\n",
    "- **定义**：允许函数接受任意数量的位置参数和/或关键字参数。\n",
    "- **特点**：增加了函数的灵活性。\n",
    "- **分类**：\n",
    "  - `*args`：捕获所有未明确指定的位置参数，作为元组传递。使用场景：当你不确定函数需要多少个位置参数时，或者希望函数能够接受任意数量的位置参数。\n",
    "  - `**kwargs`：捕获所有未明确指定的关键字参数，作为字典传递。使用场景：当你不确定函数需要多少个关键字参数时，或者希望函数能够接受任意数量的关键字参数。\n",
    "- **示例**：\n",
    "```python\n",
    "def greet(*args, **kwargs):\n",
    "    if args:\n",
    "        for arg in args:\n",
    "            print(f\"Positional argument: {arg}\")\n",
    "    if kwargs:\n",
    "        for key, value in kwargs.items():\n",
    "            print(f\"Keyword argument: {key} = {value}\")\n",
    "\n",
    "greet(\"Alice\", \"Bob\", greeting=\"Hi\", farewell=\"Goodbye\")\n",
    "# 输出:\n",
    "# Positional argument: Alice\n",
    "# Positional argument: Bob\n",
    "# Keyword argument: greeting = Hi\n",
    "# Keyword argument: farewell = Goodbye\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 解包&传参\n",
    "在 Python 中，**dict 是一种用于解包字典的语法，它允许你将字典中的键值对作为关键字参数传递给函数或方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\n",
    "```\n",
    "由make的函数签名可知：\n",
    "- `args_cli.task`是位置参数，匹配给第一个参数`id`\n",
    "- `cfg=env_cfg` 和 `render_mode=\"rgb_array\"` 是可变长度参数，会被捕获到字典中，即 kwargs = {\"cfg\": env_cfg, \"render_mode\": \"rgb_array\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gymnasium/envs/registration.py\n",
    "```python\n",
    "def make(\n",
    "    id: str | EnvSpec,\n",
    "    max_episode_steps: int | None = None,\n",
    "    autoreset: bool | None = None,\n",
    "    apply_api_compatibility: bool | None = None,\n",
    "    disable_env_checker: bool | None = None,\n",
    "    **kwargs: Any,\n",
    ") -> Env:\n",
    "    # Get the env spec\n",
    "    # env_spec = id | env_spec = _find_spec(id)\n",
    "    # ...\n",
    "    # Update the env spec kwargs with the `make` kwargs\n",
    "    env_spec_kwargs = copy.deepcopy(env_spec.kwargs)\n",
    "    # 前面的 `cfg=env_cfg` 和 `render_mode=\"rgb_array\"` 被捕获到字典中，即 kwargs = {\"cfg\": env_cfg, \"render_mode\": \"rgb_array\"}\n",
    "    # 再更新到字典 env_spec_kwargs 中\n",
    "    env_spec_kwargs.update(kwargs)\n",
    "    # Load the environment creator\n",
    "    # env_creator = env_spec.entry_point | env_creator = _load_env_creator(env_spec.entry_point)\n",
    "    # env_creator = custom_env_module.CustomEnv .e.g \n",
    "    # ...\n",
    "    env = env_creator(**env_spec_kwargs)  # 使用解包后的 env_spec_kwargs 创建环境实例\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxEnv构造函数签名\n",
    "```python\n",
    "def __init__(self, cfg: ZbotBEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "```\n",
    "- **`cfg`**：位置参数，类型为 `ZbotBEnvCfg`，是必须传递的配置对象。\n",
    "- **`render_mode`**：关键字参数，默认值为 `None`，表示渲染模式，可以省略。\n",
    "- **`**kwargs`**：可变长度关键字参数，捕获所有其他未明确指定的关键字参数。\n",
    "\n",
    "\n",
    "当调用构造函数，并**使用字典解包传递参数**时，Python 会根据参数名自动匹配到方法签名中的相应参数，并将剩余的参数放入 `kwargs` 字典中。\n",
    "\n",
    "例如：\n",
    "```python\n",
    "env = xxEnv(\n",
    "    **{\n",
    "        \"some_other_param1\": \"value1\"\n",
    "        \"cfg\": my_cfg,\n",
    "        \"render_mode\": \"rgb_array\",\n",
    "        \"some_other_param2\": value2\n",
    "    }\n",
    ")\n",
    "```\n",
    "- `cfg=my_cfg` 匹配到 `cfg` 参数。\n",
    "- `render_mode=\"rgb_array\"` 匹配到 `render_mode` 参数。\n",
    "- `some_other_param1=\"value1\"` 、`some_other_param2=value2`捕获到 `kwargs` 字典中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts/rsl_rl/train.py\n",
    "```python\n",
    "def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\n",
    "    # ...\n",
    "    # specify directory for logging experiments\n",
    "    log_root_path = os.path.join(\"logs\", \"rsl_rl\", agent_cfg.experiment_name)\n",
    "    log_root_path = os.path.abspath(log_root_path)\n",
    "    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\n",
    "    # specify directory for logging runs: {time-stamp}_{run_name}\n",
    "    log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    if agent_cfg.run_name:\n",
    "        log_dir += f\"_{agent_cfg.run_name}\"\n",
    "    log_dir = os.path.join(log_root_path, log_dir)\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.OnPolicyRunner\n",
    "runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class RslRlOnPolicyRunnerCfg:\n",
    "    \"\"\"Configuration of the runner for on-policy algorithms.\"\"\"\n",
    "\n",
    "    seed: int = 42\n",
    "    \"\"\"The seed for the experiment. Default is 42.\"\"\"\n",
    "\n",
    "    device: str = \"cuda:0\"\n",
    "    \"\"\"The device for the rl-agent. Default is cuda:0.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以 agent_cfg.device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "line 29、93 `obs, extras = self.env.get_observations()`其中get_observations()这个attribute\n",
    "\n",
    "是由于train.py中`env = RslRlVecEnvWrapper(env)`进行了wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ~/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/utils/wrappers/rsl_rl/vecenv_wrapper.py\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    # ...\n",
    "    def get_observations(self) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"Returns the current observations of the environment.\"\"\"\n",
    "        if hasattr(self.unwrapped, \"observation_manager\"):\n",
    "            obs_dict = self.unwrapped.observation_manager.compute()\n",
    "        else:\n",
    "            obs_dict = self.unwrapped._get_observations()\n",
    "        return obs_dict[\"policy\"], {\"observations\": obs_dict}\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".e.g例如\n",
    "```python\n",
    "class ZbotBEnv(DirectRLEnv):\n",
    "    cfg: ZbotBEnvCfg\n",
    "    def __init__(self, cfg: ZbotBEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        super().__init__(cfg, render_mode, **kwargs)\n",
    "        # ...\n",
    "    def _get_observations(self) -> dict:\n",
    "        obs = torch.cat(\n",
    "            (\n",
    "                self.body_quat[:,0].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self.body_quat[:,3].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self.body_quat[:,6].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self._commands,\n",
    "                self.joint_vel,\n",
    "                self.joint_pos,\n",
    "                # 4*(3)+3+6+6\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        observations = {\"policy\": obs}\n",
    "        return observations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 init_at_random_ep_len\n",
    "line 89 `init_at_random_ep_len` 在train.py中传入:\n",
    "\n",
    "`runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)`\n",
    "```python\n",
    "    episode_length_buf: torch.Tensor  # current episode duration\n",
    "    \"\"\"Buffer for current episode lengths.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        # ...\n",
    "        self.episode_length_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class ZbotBEnv(DirectRLEnv):\n",
    "    # ...\n",
    "    def _reset_idx(self, env_ids: torch.Tensor | None):\n",
    "        if env_ids is None or len(env_ids) == self.num_envs:\n",
    "            env_ids = self.zbots._ALL_INDICES\n",
    "        self.zbots.reset(env_ids)\n",
    "        super()._reset_idx(env_ids)\n",
    "        if len(env_ids) == self.num_envs:\n",
    "            # Spread out the resets to avoid spikes in training when many environments reset at a similar time\n",
    "            # 分散重置以避免在许多环境同时重置时出现训练峰值\n",
    "            self.episode_length_buf[:] = torch.randint_like(\n",
    "                self.episode_length_buf, high=int(self.max_episode_length)\n",
    "            )\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class OnPolicyRunner:\n",
    "    # ...\n",
    "    def learn(self, num_learning_iterations: int, init_at_random_ep_len: bool = False):\n",
    "        # ...\n",
    "        if init_at_random_ep_len:\n",
    "            self.env.episode_length_buf = torch.randint_like(\n",
    "                self.env.episode_length_buf, high=int(self.env.max_episode_length)\n",
    "            )\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "line109 `with torch.inference_mode()` \n",
    "\n",
    "在 rollout 阶段，模型生成动作并与环境交互，目的是收集数据以供后续训练使用。此时禁用梯度计算，可以优化性能并确保数据收集过程的可靠性。\n",
    "\n",
    "其中：\n",
    "- `self.alg.act(obs, critic_obs)` 生成动作并与环境交互。\n",
    "- `self.env.step(actions)` 执行动作并返回新的观测、奖励、完成标志和其他信息。\n",
    "- `self.obs_normalizer(obs)` 和 `self.critic_obs_normalizer(infos[\"observations\"][\"critic\"])` 对观测进行归一化处理。\n",
    "- `self.alg.process_env_step(rewards, dones, infos)` 处理环境反馈，准备后续的训练数据。\n",
    "\n",
    "训练数据的收集真正靠的是`self.alg.process_env_step(rewards, dones, infos)`传入，\n",
    "\n",
    "而外部的这些变量更多是用于tensorboard和terminal的展示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5\n",
    "line 112 `obs, rewards, dones, infos = self.env.step(actions)`使用四个参数接收返回值，也是因为`RslRlVecEnvWrapper()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原本有5个返回值\n",
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def step(self, action: torch.Tensor) -> VecEnvStepReturn:\n",
    "        \"\"\"Returns:\n",
    "            A tuple containing the observations, rewards, resets (terminated and truncated) and extras.\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        # return observations, rewards, resets and extras\n",
    "        return self.obs_buf, self.reward_buf, self.reset_terminated, self.reset_time_outs, self.extras\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    #...\n",
    "    def step(self, actions: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "        # record step information\n",
    "        obs_dict, rew, terminated, truncated, extras = self.env.step(actions)\n",
    "        # compute dones for compatibility with RSL-RL\n",
    "        dones = (terminated | truncated).to(dtype=torch.long)\n",
    "        # move extra observations to the extras dict\n",
    "        obs = obs_dict[\"policy\"]\n",
    "        extras[\"observations\"] = obs_dict\n",
    "        # move time out information to the extras dict\n",
    "        # this is only needed for infinite horizon tasks\n",
    "        if not self.unwrapped.cfg.is_finite_horizon:\n",
    "            extras[\"time_outs\"] = truncated\n",
    "\n",
    "        # return the step information\n",
    "        return obs, rew, dones, extras\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6\n",
    "```python\n",
    "rewbuffer.extend(cur_reward_sum[new_ids][:, 0].cpu().numpy().tolist())\n",
    "lenbuffer.extend(cur_episode_length[new_ids][:, 0].cpu().numpy().tolist())\n",
    "```\n",
    "如果是比较稳定没有terminate了的话，大多数env达到max_episode_length，即time_out时，才会reset。\n",
    "\n",
    "`max_episode_length = math.ceil(self.cfg.episode_length_s / (self.cfg.sim.dt * self.cfg.decimation))` 1000 = 20 / (1 / 200 * 4) .e.g\n",
    "\n",
    "所以后面mean_episode_length ≈ max_episode_length。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 cur_reward_sum[new_ids][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nonzero(as_tuple=False)\n",
    "`new_ids = (dones > 0).nonzero(as_tuple=False)`，`as_tuple=False`表示返回结果是一个二维张量，而不是一个元组。\n",
    "\n",
    "要么就像这样：`reset_env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)`（在`DirectRLEnv.step()`中）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一维张量，nonzero 返回的张量的形状是 (N, 1)，其中 N 是非零元素的数量。\n",
    "```python\n",
    "print(cur_reward_sum.shape)  # torch.Size([64])\n",
    "print(new_ids.shape)  # torch.Size([2, 1])\n",
    "print(new_ids)  # tensor([[1], [3]], device='cuda:0')\n",
    "print(cur_reward_sum[new_ids].shape)  # torch.Size([2, 1])  # 升维了\n",
    "```\n",
    "通过高级索引，改变了shape，从一维变成了二维，因此`cur_reward_sum[new_ids][:, 0]`才没抛出异常。（不能对一维进行[:, 0]）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级索引（Advanced Indexing）\n",
    "\n",
    "高级索引允许你使用整数数组来选择张量中的元素。高级索引的行为与基本切片（basic slicing）不同，它会返回一个与**索引数组形状相同的新张量**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "tensor([[-2.1058],\n",
      "        [ 0.1667],\n",
      "        [ 1.7387],\n",
      "        [-0.5361]], device='cuda:0')\n",
      "torch.Size([4, 1])\n",
      "tensor([[-2.1058,  0.1667],\n",
      "        [ 1.7387, -0.5361],\n",
      "        [ 1.7387, -0.5361],\n",
      "        [ 1.7387, -0.5361]], device='cuda:0')\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 初始化 A 和 new_ids\n",
    "A = torch.randn(64, dtype=torch.float, device='cuda')\n",
    "print(A.shape)  # 输出: torch.Size([64])\n",
    "new_ids = torch.tensor([[1], [3], [6], [8]], dtype=torch.long, device='cuda')\n",
    "# 计算 A[new_ids]\n",
    "result = A[new_ids]\n",
    "print(result)\n",
    "print(result.shape)  # 输出: torch.Size([4, 1])\n",
    "\n",
    "new_ids_2 = torch.tensor([[1, 3], [6, 8], [6, 8], [6, 8]], dtype=torch.long, device='cuda')\n",
    "result_2 = A[new_ids_2]\n",
    "print(result_2)\n",
    "print(result_2.shape)  # 输出: torch.Size([4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 deque\n",
    "虽然rewbuffer每个step都在更新(除非某一步所有env都没有reset)，但每个iteration(16steps .e.g)才计算rewbuffer平均值并更log一次\n",
    "\n",
    "同时rewbuffer的maxlen=100，前期一个step的reset的env数量多，可能都不够装，只能记录后100个reset的env的值\n",
    "\n",
    "也就是说log时，可能只用到了一轮iteration中最后一个step的后100个reset_env的值，来计算平均值\n",
    "\n",
    "当然，通过前面提到的`init_at_random_ep_len`机制，reset的env分散了，不会总是固定的那100个env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], maxlen=100)\n",
      "1\n",
      "deque([0, 1, 2], maxlen=3)\n",
      "deque([2, 3, 4], maxlen=3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import statistics\n",
    "\n",
    "rewbuffer = deque(maxlen=100)\n",
    "rewbuffer.extend([1, 1, 1, 1])\n",
    "rewbuffer.extend([1, 1, 1, 1, 1])\n",
    "rewbuffer.extend([1, 1, 1])\n",
    "print(rewbuffer)\n",
    "print(statistics.mean(rewbuffer))\n",
    "\n",
    "small_buffer = deque(maxlen=3)\n",
    "small_buffer.extend([-1, 0, 1, 2])\n",
    "print(small_buffer)\n",
    "small_buffer.extend([3, 4])\n",
    "print(small_buffer)\n",
    "print(statistics.mean(small_buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7\n",
    "line154 `self.log(locals())`， log的详解见#3\n",
    "\n",
    "↓\n",
    "\n",
    "line205 `self.writer.add_scalar(\"Train/mean_reward\", statistics.mean(locs[\"rewbuffer\"]), locs[\"it\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 statistics.mean()\n",
    "通过 `_sum` 函数计算总和与元素个数，并使用 `_convert` 函数确保结果类型与输入一致。如分数 (Fraction) 和十进制数 (Decimal)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def mean(data):\n",
    "    \"\"\"Return the sample arithmetic mean of data.\n",
    "\n",
    "    >>> mean([1, 2, 3, 4, 4])\n",
    "    2.8\n",
    "\n",
    "    >>> from fractions import Fraction as F\n",
    "    >>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\n",
    "    Fraction(13, 21)\n",
    "\n",
    "    >>> from decimal import Decimal as D\n",
    "    >>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\n",
    "    Decimal('0.5625')\n",
    "\n",
    "    If ``data`` is empty, StatisticsError will be raised.\n",
    "    \"\"\"\n",
    "    if iter(data) is data:\n",
    "        data = list(data)\n",
    "    n = len(data)\n",
    "    if n < 1:\n",
    "        raise StatisticsError('mean requires at least one data point')\n",
    "    T, total, count = _sum(data)\n",
    "    assert count == n\n",
    "    return _convert(total / n, T)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 迭代器对象和可迭代对象的区别\n",
    "\n",
    "在 Python 中，**迭代器对象** 和 **可迭代对象** 是两个不同的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 可迭代对象（Iterable）\n",
    "- **定义**：可迭代对象是实现了 `__iter__()` 方法的对象。这个方法返回一个迭代器对象。\n",
    "- **特点**：\n",
    "  - 可以通过 `for` 循环遍历。\n",
    "  - 可以传递给内置函数 `iter()` 来获取迭代器对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **常见类型**：\n",
    "  - 列表 (`list`)\n",
    "  - 元组 (`tuple`)\n",
    "  - 字符串 (`str`)\n",
    "  - 字典 (`dict`)\n",
    "  - 集合 (`set`)\n",
    "  - 文件对象\n",
    "  - 自定义的可迭代类\n",
    "\n",
    "**示例**：\n",
    "```python\n",
    "# 列表是一个可迭代对象\n",
    "my_list = [1, 2, 3]\n",
    "for item in my_list:\n",
    "    print(item)\n",
    "\n",
    "# 使用 iter() 获取迭代器对象\n",
    "iterator = iter(my_list)  # 大多数情况下，iter(my_list) 等价于 my_list.__iter__()\n",
    "print(next(iterator))  # 输出 1\n",
    "print(next(iterator))  # 输出 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 迭代器对象（Iterator）\n",
    "- **定义**：迭代器对象是实现了 `__next__()` 方法的对象。每次调用 `next()` 函数时，它会返回下一个值，直到没有更多元素为止，此时会抛出 `StopIteration` 异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **特点**：\n",
    "  - 必须实现 `__iter__()` 方法，返回自身（即 `self`）。\n",
    "  - 每次调用 `next()` 返回序列中的下一个值。\n",
    "  - 一旦遍历结束，再次调用 `next()` 会抛出 `StopIteration` 异常。\n",
    "- **常见类型**：\n",
    "  - 由 `iter()` 函数从可迭代对象生成的对象。\n",
    "  - 生成器（Generator）\n",
    "\n",
    "**示例**：\n",
    "```python\n",
    "# 创建一个迭代器对象\n",
    "my_iterator = iter([1, 2, 3])\n",
    "\n",
    "# 使用 next() 获取下一个值\n",
    "print(next(my_iterator))  # 输出 1\n",
    "print(next(my_iterator))  # 输出 2\n",
    "print(next(my_iterator))  # 输出 3\n",
    "# print(next(my_iterator))  # 抛出 StopIteration 异常\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 区别与联系\n",
    "- **可迭代对象** 是可以被迭代的对象，但不一定能直接调用 `next()` 方法。\n",
    "- **迭代器对象** 是专门用于迭代的对象，可以直接调用 `next()` 方法。\n",
    "- **关系**：每个可迭代对象都可以通过 `iter()` 函数转换为迭代器对象。迭代器对象本身也是一个可迭代对象，因为它实现了 `__iter__()` 方法并返回自身。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 特殊情况：`if iter(data) is data`\n",
    "- **解释**：这行代码检查 `data` 是否既是可迭代对象又是迭代器对象，并且 `iter(data)` 返回的就是 `data` 本身。\n",
    "- **应用场景**：通常用于判断 `data` 是否已经是迭代器对象，而不是普通的可迭代对象。例如，在某些情况下，你可能需要传入可迭代对象作为参数，避免重复创建迭代器对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Extras & Tensorboard logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 extras 的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        # ...\n",
    "        # allocate dictionary to store metrics\n",
    "        self.extras = {}\n",
    "        # ...\n",
    "    def reset(self, seed: int | None = None, options: dict[str, Any] | None = None) -> tuple[VecEnvObs, dict]:\n",
    "        # ...\n",
    "        # reset state of scene\n",
    "        indices = torch.arange(self.num_envs, dtype=torch.int64, device=self.device)\n",
    "        self._reset_idx(indices)  # 传入indices有点奇怪，不知道哪里会使用这个reset函数，后面发现被RslRlVecEnvWrapper()重写了\n",
    "        # ...\n",
    "        # return observations\n",
    "        return self._get_observations(), self.extras\n",
    "    def step(self, action: torch.Tensor) -> VecEnvStepReturn:\n",
    "        \"\"\"Returns:\n",
    "            A tuple containing the observations, rewards, resets (terminated and truncated) and extras.\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        # return observations, rewards, resets and extras\n",
    "        return self.obs_buf, self.reward_buf, self.reset_terminated, self.reset_time_outs, self.extras\n",
    "    # ...\n",
    "    def _reset_idx(self, env_ids: Sequence[int]):\n",
    "        \"\"\"Reset environments based on specified indices.\n",
    "        \"\"\"\n",
    "        self.scene.reset(env_ids)\n",
    "        # ...\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 为extras添加自定义的信息\n",
    "```python\n",
    "self.extras[\"log\"] = dict()\n",
    "self.extras[\"log\"].update(your_log_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".e.g\n",
    "```python\n",
    "class AnymalCEnv(DirectRLEnv):\n",
    "    cfg: AnymalCFlatEnvCfg | AnymalCRoughEnvCfg\n",
    "\n",
    "    def __init__(self, cfg: AnymalCFlatEnvCfg | AnymalCRoughEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        super().__init__(cfg, render_mode, **kwargs)\n",
    "        # ...\n",
    "        # Logging\n",
    "        self._episode_sums = {\n",
    "            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)\n",
    "            for key in [\n",
    "                \"track_lin_vel_xy_exp\",\n",
    "                \"track_ang_vel_z_exp\",\n",
    "                \"lin_vel_z_l2\",\n",
    "                \"ang_vel_xy_l2\",\n",
    "                \"dof_torques_l2\",\n",
    "                \"dof_acc_l2\",\n",
    "                \"action_rate_l2\",\n",
    "                \"feet_air_time\",\n",
    "                \"undesired_contacts\",\n",
    "                \"flat_orientation_l2\",\n",
    "            ]\n",
    "        }\n",
    "        # ...\n",
    "    # ...\n",
    "    def _get_rewards(self) -> torch.Tensor:\n",
    "        # ...\n",
    "        rewards = {\n",
    "            \"track_lin_vel_xy_exp\": lin_vel_error_mapped * self.cfg.lin_vel_reward_scale * self.step_dt,\n",
    "            \"track_ang_vel_z_exp\": yaw_rate_error_mapped * self.cfg.yaw_rate_reward_scale * self.step_dt,\n",
    "            \"lin_vel_z_l2\": z_vel_error * self.cfg.z_vel_reward_scale * self.step_dt,\n",
    "            \"ang_vel_xy_l2\": ang_vel_error * self.cfg.ang_vel_reward_scale * self.step_dt,\n",
    "            \"dof_torques_l2\": joint_torques * self.cfg.joint_torque_reward_scale * self.step_dt,\n",
    "            \"dof_acc_l2\": joint_accel * self.cfg.joint_accel_reward_scale * self.step_dt,\n",
    "            \"action_rate_l2\": action_rate * self.cfg.action_rate_reward_scale * self.step_dt,\n",
    "            \"feet_air_time\": air_time * self.cfg.feet_air_time_reward_scale * self.step_dt,\n",
    "            \"undesired_contacts\": contacts * self.cfg.undersired_contact_reward_scale * self.step_dt,\n",
    "            \"flat_orientation_l2\": flat_orientation * self.cfg.flat_orientation_reward_scale * self.step_dt,\n",
    "        }\n",
    "        reward = torch.sum(torch.stack(list(rewards.values())), dim=0)\n",
    "        # Logging\n",
    "        for key, value in rewards.items():\n",
    "            self._episode_sums[key] += value\n",
    "        return reward\n",
    "    # ...\n",
    "    def _reset_idx(self, env_ids: torch.Tensor | None):\n",
    "        # ...\n",
    "        # Logging\n",
    "        extras = dict()\n",
    "        for key in self._episode_sums.keys():\n",
    "            episodic_sum_avg = torch.mean(self._episode_sums[key][env_ids])\n",
    "            extras[\"Episode_Reward/\" + key] = episodic_sum_avg / self.max_episode_length_s\n",
    "            self._episode_sums[key][env_ids] = 0.0\n",
    "        self.extras[\"log\"] = dict()\n",
    "        self.extras[\"log\"].update(extras)\n",
    "        extras = dict()\n",
    "        extras[\"Episode_Termination/base_contact\"] = torch.count_nonzero(self.reset_terminated[env_ids]).item()\n",
    "        extras[\"Episode_Termination/time_out\"] = torch.count_nonzero(self.reset_time_outs[env_ids]).item()\n",
    "        self.extras[\"log\"].update(extras)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 extras 的包装和改造\n",
    "`extras[\"log\"]` 和 `extras[\"observations\"]`、`extras[\"time_outs\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ~/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/utils/wrappers/rsl_rl/vecenv_wrapper.py\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    # ...\n",
    "    def reset(self) -> tuple[torch.Tensor, dict]:  # noqa: D102  # 忽略缺少公共方法文档字符串的警告\n",
    "        # reset the environment\n",
    "        obs_dict, _ = self.env.reset()  # 前面定义的extra被丢弃了\n",
    "        # return observations\n",
    "        return obs_dict[\"policy\"], {\"observations\": obs_dict}\n",
    "    \n",
    "    def step(self, actions: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "        # record step information\n",
    "        obs_dict, rew, terminated, truncated, extras = self.env.step(actions)\n",
    "        # compute dones for compatibility with RSL-RL\n",
    "        dones = (terminated | truncated).to(dtype=torch.long)\n",
    "        # move extra observations to the extras dict\n",
    "        obs = obs_dict[\"policy\"]\n",
    "        extras[\"observations\"] = obs_dict\n",
    "        # move time out information to the extras dict\n",
    "        # this is only needed for infinite horizon tasks\n",
    "        if not self.unwrapped.cfg.is_finite_horizon:\n",
    "            extras[\"time_outs\"] = truncated\n",
    "        # return the step information\n",
    "        return obs, rew, dones, extras\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 in OnPolicyRunner\n",
    "`ep_infos = []`，每一个step都有 `ep_infos.append(infos[\"log\"])`\n",
    "\n",
    "但是一轮iteration（16steps .e.g）才会`self.log(locals())`，随后`ep_infos.clear()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class OnPolicyRunner:\n",
    "    #...\n",
    "    def log(self, locs: dict, width: int = 80, pad: int = 35):\n",
    "        self.tot_timesteps += self.num_steps_per_env * self.env.num_envs\n",
    "        self.tot_time += locs[\"collection_time\"] + locs[\"learn_time\"]\n",
    "        iteration_time = locs[\"collection_time\"] + locs[\"learn_time\"]\n",
    "\n",
    "        ep_string = \"\"\n",
    "        if locs[\"ep_infos\"]:\n",
    "            for key in locs[\"ep_infos\"][0]:\n",
    "                infotensor = torch.tensor([], device=self.device)  # 对于每个键，初始化一个空的张量infotensor，用于存储该键对应的值\n",
    "                for ep_info in locs[\"ep_infos\"]:  # 遍历每个step，len = num_steps_per_env (16 .e.g)\n",
    "                    # handle scalar and zero dimensional tensor infos\n",
    "                    if key not in ep_info:\n",
    "                        continue  # 如果当前step信息中不存在该键，则跳过\n",
    "                    if not isinstance(ep_info[key], torch.Tensor):\n",
    "                        ep_info[key] = torch.Tensor([ep_info[key]])  # 如果该键对应的值不是张量，则将其转换为张量\n",
    "                    if len(ep_info[key].shape) == 0:\n",
    "                        ep_info[key] = ep_info[key].unsqueeze(0)  # 如果该键对应的是零维张量（标量），则升为一维张量，确保后续的cat能够正常工作\n",
    "                    infotensor = torch.cat((infotensor, ep_info[key].to(self.device)))  # 将该键每个step对应的值与infotensor进行拼接\n",
    "                value = torch.mean(infotensor)  # 计算该键本轮所有step的平均值，所以一些键本应是整数的值，成了小数\n",
    "                # log to logger and terminal\n",
    "                if \"/\" in key:\n",
    "                    self.writer.add_scalar(key, value, locs[\"it\"])\n",
    "                    ep_string += f\"\"\"{f'{key}:':>{pad}} {value:.4f}\\n\"\"\"\n",
    "                else:\n",
    "                    self.writer.add_scalar(\"Episode/\" + key, value, locs[\"it\"])\n",
    "                    ep_string += f\"\"\"{f'Mean episode {key}:':>{pad}} {value:.4f}\\n\"\"\"\n",
    "        # ...\n",
    "        # 一些默认统计信息的计算和输出\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ep_infos = []\n",
    "if ep_infos:\n",
    "    print(\"ep_infos = true\")\n",
    "else:\n",
    "    print(ep_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 零维张量（0-dimensional tensor）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中零维张量也称为标量张量Scalar tensor。零维张量没有形状（shape），通常用于表示单个数值。\n",
    "\n",
    "**零维张量的特点：**\n",
    "- **形状**：`torch.Size([])`，表示没有维度。\n",
    "- **大小**：包含一个单一的值。\n",
    "- **操作**：可以像普通标量一样进行数学运算，并且可以通过 `.item()` 方法获取其内部的 Python 标量值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar tensor: 3.140000104904175\n",
      "Shape: torch.Size([])\n",
      "Value: 3.140000104904175\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个零维张量\n",
    "scalar_tensor = torch.tensor(3.14)\n",
    "print(f\"Scalar tensor: {scalar_tensor}\")\n",
    "print(f\"Shape: {scalar_tensor.shape}\")  # 输出: Shape: torch.Size([])\n",
    "print(f\"Value: {scalar_tensor.item()}\")  # 输出: Value: 3.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述代码中\n",
    "```python\n",
    "if len(ep_info[key].shape) == 0:\n",
    "    ep_info[key] = ep_info[key].unsqueeze(0)\n",
    "```\n",
    "检查 `ep_info[key]` 是否为零维张量，如果是，则使用 `unsqueeze(0)` 方法为其增加一个维度，使其变为一维张量。这一步骤是为了确保后续的拼接操作（如 `torch.cat`）能够正常工作，因为 `torch.cat` 需要所有输入张量具有相同的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 嵌套的格式化字符串\n",
    "`ep_string += f\"\"\"{f'{key}:':>{pad}} {value:.4f}\\n\"\"\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 内层的 `f'...':>{pad}`，用于将变量插入到字符串中\n",
    "- 外层的 `f\"\"\"...\"\"\"`，用于进一步格式化字符串\n",
    "  - `:>{pad}` 右对齐并填充空白字符，使整个字符串的宽度达到pad指定的长度，默认右对齐，即`:{pad}`\n",
    "  - 左对齐填充*，则`:*<{pad}`\n",
    "  - `{value:.4f}` 将 value 变量格式化为浮点数，保留四位小数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2\n",
    "默认的log信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss\n",
    "  - Loss/value_function `mean_value_loss, mean_surrogate_loss = self.alg.update()`\n",
    "  - Loss/surrogate `mean_value_loss, mean_surrogate_loss = self.alg.update()`\n",
    "  - Loss/learning_rate `self.alg.learning_rate`\n",
    "- Policy\n",
    "  - Policy/mean_noise_std `self.alg.actor_critic.std.mean().item()`\n",
    "- Perf\n",
    "  - Perf/total_fps `int(self.num_steps_per_env * self.env.num_envs / (collection_time + learn_time))`\n",
    "  - Perf/collection time `collection_time`\n",
    "  - Perf/learning_time `learn_time`\n",
    "- Train\n",
    "  - Train/mean_reward `statistics.mean(rewbuffer)`\n",
    "  - Train/mean_episode_length `statistics.mean(lenbuffer)`\n",
    "  - Train/mean_reward/time `statistics.mean(rewbuffer)`  横坐标不再是轮次`it`，而是时间`tot_time`\n",
    "  - Train/mean_episode_length/time `statistics.mean(lenbuffer)`  横坐标不再是轮次`it`，而是时间`tot_time`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal log_string += ep_string\n",
    "\n",
    "log_sting += ↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal额外的信息：\n",
    "- Total timesteps: `self.tot_timesteps += self.num_steps_per_env * self.env.num_envs`\n",
    "- Iteration time: `iteration_time = collection_time + learn_time`\n",
    "- Total time: `self.tot_time += collection_time + learn_time`\n",
    "- ETA: `self.tot_time / (it + 1) * (num_learning_iterations - it)` 估计剩余时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class OnPolicyRunner:\n",
    "    \"\"\"On-policy runner for training and evaluation.\"\"\"\n",
    "\n",
    "    def __init__(self, env: VecEnv, train_cfg, log_dir=None, device=\"cpu\"):\n",
    "        self.cfg = train_cfg  # agent_cfg.to_dict() `runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)`\n",
    "        self.alg_cfg = train_cfg[\"algorithm\"]\n",
    "        self.policy_cfg = train_cfg[\"policy\"]\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "        obs, extras = self.env.get_observations()\n",
    "        num_obs = obs.shape[1]\n",
    "        if \"critic\" in extras[\"observations\"]:\n",
    "            num_critic_obs = extras[\"observations\"][\"critic\"].shape[1]\n",
    "        else:\n",
    "            num_critic_obs = num_obs\n",
    "        actor_critic_class = eval(self.policy_cfg.pop(\"class_name\"))  # ActorCritic\n",
    "        actor_critic: ActorCritic | ActorCriticRecurrent = actor_critic_class(\n",
    "            num_obs, num_critic_obs, self.env.num_actions, **self.policy_cfg\n",
    "        ).to(self.device)\n",
    "        alg_class = eval(self.alg_cfg.pop(\"class_name\"))  # PPO\n",
    "        self.alg: PPO = alg_class(actor_critic, device=self.device, **self.alg_cfg)\n",
    "        self.num_steps_per_env = self.cfg[\"num_steps_per_env\"]\n",
    "        self.save_interval = self.cfg[\"save_interval\"]\n",
    "        self.empirical_normalization = self.cfg[\"empirical_normalization\"]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
