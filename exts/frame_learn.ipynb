{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Register\n",
    "the env_cfg and agent_cfg is how to convey in this framework\n",
    "\n",
    "- 首先要有个概念：我们的env_cfg继承了DirectRLEnvCfg，agent_cfg继承了RslRlOnPolicyRunnerCfg，也就继承了一些方法（函数）和一些默认属性property。当然也有些方法只有声明，没有实现implement，例如the '_get_observations' method就是在我们的env_cfg中具体实现的。\n",
    "\n",
    "- 然后，由env_cfg创建的env又通过RslRlVecEnvWrapper()进行了wrap\n",
    "- 最后传入了OnPolicyRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts/rsl_rl/train.py\n",
    "```python\n",
    "@hydra_task_config(args_cli.task, \"rsl_rl_cfg_entry_point\")\n",
    "def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\n",
    "    # ...\n",
    "    # create isaac environment\n",
    "    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\n",
    "    # ...\n",
    "    # wrap around environment for rsl-rl\n",
    "    env = RslRlVecEnvWrapper(env)\n",
    "    # create runner from rsl-rl\n",
    "    runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)\n",
    "    # ...\n",
    "    # run training\n",
    "    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)\n",
    "    # close the simulator\n",
    "    env.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓decorator 负责解析并提供 env_cfg 和 agent_cfg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def hydra_task_config(task_name: str, agent_cfg_entry_point: str) -> Callable:\n",
    "    #...\n",
    "    env_cfg, agent_cfg = register_task_to_hydra(task_name, agent_cfg_entry_point)\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def register_task_to_hydra(\n",
    "    task_name: str, agent_cfg_entry_point: str\n",
    ") -> tuple[ManagerBasedRLEnvCfg | DirectRLEnvCfg, dict]：\n",
    "    # ...\n",
    "    env_cfg = load_cfg_from_registry(task_name, \"env_cfg_entry_point\")\n",
    "    agent_cfg = load_cfg_from_registry(task_name, agent_cfg_entry_point)\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def load_cfg_from_registry(task_name: str, entry_point_key: str) -> dict | object:\n",
    "    \"\"\"It supports both YAML and Python configuration files.\n",
    "    If the entry point is a YAML file, it is parsed into a dictionary.\n",
    "    If the entry point is a Python class, it is instantiated and returned.\"\"\"\n",
    "    # obtain the configuration entry point\n",
    "    cfg_entry_point = gym.spec(task_name).kwargs.get(entry_point_key)\n",
    "    # 如果 cfg_entry_point 是一个以 .yaml 结尾的字符串\n",
    "    # 如果 cfg_entry_point 是可调用的（例如是一个函数或类）\n",
    "    # 如果 cfg_entry_point 是字符串（格式为 \"module_name:attr_name\"）\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".e.g例如\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from . import agents\n",
    "from .zbot6b_env_v0 import ZbotBEnv, ZbotBEnvCfg\n",
    "##\n",
    "# Register Gym environments.\n",
    "##\n",
    "gym.register(\n",
    "    id=\"Zbot-6b-walking-v0\",\n",
    "    entry_point=\"Zbot.tasks.moving.zbot6b_direct:ZbotBEnv\",\n",
    "    disable_env_checker=True,\n",
    "    kwargs={\n",
    "        \"env_cfg_entry_point\": ZbotBEnvCfg, \n",
    "        \"rsl_rl_cfg_entry_point\": f\"{agents.__name__}.rsl_rl_ppo_cfg:ZbotSBFlatPPORunnerCfg\",\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts/rsl_rl/train.py\n",
    "```python\n",
    "def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\n",
    "    # ...\n",
    "    # specify directory for logging experiments\n",
    "    log_root_path = os.path.join(\"logs\", \"rsl_rl\", agent_cfg.experiment_name)\n",
    "    log_root_path = os.path.abspath(log_root_path)\n",
    "    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\n",
    "    # specify directory for logging runs: {time-stamp}_{run_name}\n",
    "    log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    if agent_cfg.run_name:\n",
    "        log_dir += f\"_{agent_cfg.run_name}\"\n",
    "    log_dir = os.path.join(log_root_path, log_dir)\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.OnPolicyRunner\n",
    "runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class RslRlOnPolicyRunnerCfg:\n",
    "    \"\"\"Configuration of the runner for on-policy algorithms.\"\"\"\n",
    "\n",
    "    seed: int = 42\n",
    "    \"\"\"The seed for the experiment. Default is 42.\"\"\"\n",
    "\n",
    "    device: str = \"cuda:0\"\n",
    "    \"\"\"The device for the rl-agent. Default is cuda:0.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以 agent_cfg.device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "line 29、93 `obs, extras = self.env.get_observations()`其中get_observations()这个attribute\n",
    "\n",
    "是由于train.py中`env = RslRlVecEnvWrapper(env)`进行了wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ~/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/utils/wrappers/rsl_rl/vecenv_wrapper.py\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    # ...\n",
    "    def get_observations(self) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"Returns the current observations of the environment.\"\"\"\n",
    "        if hasattr(self.unwrapped, \"observation_manager\"):\n",
    "            obs_dict = self.unwrapped.observation_manager.compute()\n",
    "        else:\n",
    "            obs_dict = self.unwrapped._get_observations()\n",
    "        return obs_dict[\"policy\"], {\"observations\": obs_dict}\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".e.g例如\n",
    "```python\n",
    "class ZbotBEnv(DirectRLEnv):\n",
    "    cfg: ZbotBEnvCfg\n",
    "    def __init__(self, cfg: ZbotBEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        super().__init__(cfg, render_mode, **kwargs)\n",
    "        # ...\n",
    "    def _get_observations(self) -> dict:\n",
    "        obs = torch.cat(\n",
    "            (\n",
    "                self.body_quat[:,0].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self.body_quat[:,3].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self.body_quat[:,6].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self._commands,\n",
    "                self.joint_vel,\n",
    "                self.joint_pos,\n",
    "                # 4*(3)+3+6+6\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        observations = {\"policy\": obs}\n",
    "        return observations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 init_at_random_ep_len\n",
    "line 89 `init_at_random_ep_len` 在train.py中传入:\n",
    "\n",
    "`runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)`\n",
    "```python\n",
    "    episode_length_buf: torch.Tensor  # current episode duration\n",
    "    \"\"\"Buffer for current episode lengths.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        # ...\n",
    "        self.episode_length_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class ZbotBEnv(DirectRLEnv):\n",
    "    # ...\n",
    "    def _reset_idx(self, env_ids: torch.Tensor | None):\n",
    "        if env_ids is None or len(env_ids) == self.num_envs:\n",
    "            env_ids = self.zbots._ALL_INDICES\n",
    "        self.zbots.reset(env_ids)\n",
    "        super()._reset_idx(env_ids)\n",
    "        if len(env_ids) == self.num_envs:\n",
    "            # Spread out the resets to avoid spikes in training when many environments reset at a similar time\n",
    "            # 分散重置以避免在许多环境同时重置时出现训练峰值\n",
    "            self.episode_length_buf[:] = torch.randint_like(\n",
    "                self.episode_length_buf, high=int(self.max_episode_length)\n",
    "            )\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class OnPolicyRunner:\n",
    "    # ...\n",
    "    def learn(self, num_learning_iterations: int, init_at_random_ep_len: bool = False):\n",
    "        # ...\n",
    "        if init_at_random_ep_len:\n",
    "            self.env.episode_length_buf = torch.randint_like(\n",
    "                self.env.episode_length_buf, high=int(self.env.max_episode_length)\n",
    "            )\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "line109 `with torch.inference_mode()` \n",
    "\n",
    "在 rollout 阶段，模型生成动作并与环境交互，目的是收集数据以供后续训练使用。此时禁用梯度计算，可以优化性能并确保数据收集过程的可靠性。\n",
    "\n",
    "其中：\n",
    "- `self.alg.act(obs, critic_obs)` 生成动作并与环境交互。\n",
    "- `self.env.step(actions)` 执行动作并返回新的观测、奖励、完成标志和其他信息。\n",
    "- `self.obs_normalizer(obs)` 和 `self.critic_obs_normalizer(infos[\"observations\"][\"critic\"])` 对观测进行归一化处理。\n",
    "- `self.alg.process_env_step(rewards, dones, infos)` 处理环境反馈，准备后续的训练数据。\n",
    "\n",
    "训练数据的收集真正靠的是`self.alg.process_env_step(rewards, dones, infos)`传入，\n",
    "\n",
    "而外部的这些变量更多是用于tensorboard和terminal的展示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5\n",
    "line 112 `obs, rewards, dones, infos = self.env.step(actions)`使用四个参数接收返回值，也是因为`RslRlVecEnvWrapper()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原本有5个返回值\n",
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def step(self, action: torch.Tensor) -> VecEnvStepReturn:\n",
    "        \"\"\"Returns:\n",
    "            A tuple containing the observations, rewards, resets (terminated and truncated) and extras.\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        # return observations, rewards, resets and extras\n",
    "        return self.obs_buf, self.reward_buf, self.reset_terminated, self.reset_time_outs, self.extras\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    #...\n",
    "    def step(self, actions: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "        # record step information\n",
    "        obs_dict, rew, terminated, truncated, extras = self.env.step(actions)\n",
    "        # compute dones for compatibility with RSL-RL\n",
    "        dones = (terminated | truncated).to(dtype=torch.long)\n",
    "        # move extra observations to the extras dict\n",
    "        obs = obs_dict[\"policy\"]\n",
    "        extras[\"observations\"] = obs_dict\n",
    "        # move time out information to the extras dict\n",
    "        # this is only needed for infinite horizon tasks\n",
    "        if not self.unwrapped.cfg.is_finite_horizon:\n",
    "            extras[\"time_outs\"] = truncated\n",
    "\n",
    "        # return the step information\n",
    "        return obs, rew, dones, extras\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6\n",
    "```python\n",
    "rewbuffer.extend(cur_reward_sum[new_ids][:, 0].cpu().numpy().tolist())\n",
    "lenbuffer.extend(cur_episode_length[new_ids][:, 0].cpu().numpy().tolist())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 cur_reward_sum[new_ids][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nonzero(as_tuple=False)\n",
    "`new_ids = (dones > 0).nonzero(as_tuple=False)`，`as_tuple=False`表示返回结果是一个二维张量，而不是一个元组。\n",
    "\n",
    "要么就像这样：`reset_env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)`（在`DirectRLEnv.step()`中）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一维张量，nonzero 返回的张量的形状是 (N, 1)，其中 N 是非零元素的数量。\n",
    "```python\n",
    "print(cur_reward_sum.shape)  # torch.Size([64])\n",
    "print(new_ids.shape)  # torch.Size([2, 1])\n",
    "print(new_ids)  # tensor([[1], [3]], device='cuda:0')\n",
    "print(cur_reward_sum[new_ids].shape)  # torch.Size([2, 1])  # 升维了\n",
    "```\n",
    "通过高级索引，改变了shape，从一维变成了二维，因此`cur_reward_sum[new_ids][:, 0]`才没抛出异常。（不能对一维进行[:, 0]）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级索引（Advanced Indexing）\n",
    "\n",
    "高级索引允许你使用整数数组来选择张量中的元素。高级索引的行为与基本切片（basic slicing）不同，它会返回一个与**索引数组形状相同的新张量**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "tensor([[-2.1058],\n",
      "        [ 0.1667],\n",
      "        [ 1.7387],\n",
      "        [-0.5361]], device='cuda:0')\n",
      "torch.Size([4, 1])\n",
      "tensor([[-2.1058,  0.1667],\n",
      "        [ 1.7387, -0.5361],\n",
      "        [ 1.7387, -0.5361],\n",
      "        [ 1.7387, -0.5361]], device='cuda:0')\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 初始化 A 和 new_ids\n",
    "A = torch.randn(64, dtype=torch.float, device='cuda')\n",
    "print(A.shape)  # 输出: torch.Size([64])\n",
    "new_ids = torch.tensor([[1], [3], [6], [8]], dtype=torch.long, device='cuda')\n",
    "# 计算 A[new_ids]\n",
    "result = A[new_ids]\n",
    "print(result)\n",
    "print(result.shape)  # 输出: torch.Size([4, 1])\n",
    "\n",
    "new_ids_2 = torch.tensor([[1, 3], [6, 8], [6, 8], [6, 8]], dtype=torch.long, device='cuda')\n",
    "result_2 = A[new_ids_2]\n",
    "print(result_2)\n",
    "print(result_2.shape)  # 输出: torch.Size([4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 deque\n",
    "虽然rewbuffer每个step都在更新(除非某一步所有env都没有reset)，但每个iteration(16steps .e.g)才计算rewbuffer平均值并更log一次\n",
    "\n",
    "同时rewbuffer的maxlen=100，前期一个step的reset的env数量多，可能都不够装，只能记录后100个reset的env的值\n",
    "\n",
    "也就是说log时，可能只用到了一轮iteration中最后一个step的后100个reset_env的值，来计算平均值\n",
    "\n",
    "当然，通过前面提到的`init_at_random_ep_len`机制，reset的env分散了，不会总是固定的那100个env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], maxlen=100)\n",
      "1\n",
      "deque([0, 1, 2], maxlen=3)\n",
      "deque([2, 3, 4], maxlen=3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import statistics\n",
    "\n",
    "rewbuffer = deque(maxlen=100)\n",
    "rewbuffer.extend([1, 1, 1, 1])\n",
    "rewbuffer.extend([1, 1, 1, 1, 1])\n",
    "rewbuffer.extend([1, 1, 1])\n",
    "print(rewbuffer)\n",
    "print(statistics.mean(rewbuffer))\n",
    "\n",
    "small_buffer = deque(maxlen=3)\n",
    "small_buffer.extend([-1, 0, 1, 2])\n",
    "print(small_buffer)\n",
    "small_buffer.extend([3, 4])\n",
    "print(small_buffer)\n",
    "print(statistics.mean(small_buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7\n",
    "line154 `self.log(locals())`， log的详解见#3\n",
    "\n",
    "↓\n",
    "\n",
    "line205 `self.writer.add_scalar(\"Train/mean_reward\", statistics.mean(locs[\"rewbuffer\"]), locs[\"it\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 statistics.mean()\n",
    "通过 `_sum` 函数计算总和与元素个数，并使用 `_convert` 函数确保结果类型与输入一致。如分数 (Fraction) 和十进制数 (Decimal)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def mean(data):\n",
    "    \"\"\"Return the sample arithmetic mean of data.\n",
    "\n",
    "    >>> mean([1, 2, 3, 4, 4])\n",
    "    2.8\n",
    "\n",
    "    >>> from fractions import Fraction as F\n",
    "    >>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\n",
    "    Fraction(13, 21)\n",
    "\n",
    "    >>> from decimal import Decimal as D\n",
    "    >>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\n",
    "    Decimal('0.5625')\n",
    "\n",
    "    If ``data`` is empty, StatisticsError will be raised.\n",
    "    \"\"\"\n",
    "    if iter(data) is data:\n",
    "        data = list(data)\n",
    "    n = len(data)\n",
    "    if n < 1:\n",
    "        raise StatisticsError('mean requires at least one data point')\n",
    "    T, total, count = _sum(data)\n",
    "    assert count == n\n",
    "    return _convert(total / n, T)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 迭代器对象和可迭代对象的区别\n",
    "\n",
    "在 Python 中，**迭代器对象** 和 **可迭代对象** 是两个不同的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 可迭代对象（Iterable）\n",
    "- **定义**：可迭代对象是实现了 `__iter__()` 方法的对象。这个方法返回一个迭代器对象。\n",
    "- **特点**：\n",
    "  - 可以通过 `for` 循环遍历。\n",
    "  - 可以传递给内置函数 `iter()` 来获取迭代器对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **常见类型**：\n",
    "  - 列表 (`list`)\n",
    "  - 元组 (`tuple`)\n",
    "  - 字符串 (`str`)\n",
    "  - 字典 (`dict`)\n",
    "  - 集合 (`set`)\n",
    "  - 文件对象\n",
    "  - 自定义的可迭代类\n",
    "\n",
    "**示例**：\n",
    "```python\n",
    "# 列表是一个可迭代对象\n",
    "my_list = [1, 2, 3]\n",
    "for item in my_list:\n",
    "    print(item)\n",
    "\n",
    "# 使用 iter() 获取迭代器对象\n",
    "iterator = iter(my_list)  # 大多数情况下，iter(my_list) 等价于 my_list.__iter__()\n",
    "print(next(iterator))  # 输出 1\n",
    "print(next(iterator))  # 输出 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 迭代器对象（Iterator）\n",
    "- **定义**：迭代器对象是实现了 `__next__()` 方法的对象。每次调用 `next()` 函数时，它会返回下一个值，直到没有更多元素为止，此时会抛出 `StopIteration` 异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **特点**：\n",
    "  - 必须实现 `__iter__()` 方法，返回自身（即 `self`）。\n",
    "  - 每次调用 `next()` 返回序列中的下一个值。\n",
    "  - 一旦遍历结束，再次调用 `next()` 会抛出 `StopIteration` 异常。\n",
    "- **常见类型**：\n",
    "  - 由 `iter()` 函数从可迭代对象生成的对象。\n",
    "  - 生成器（Generator）\n",
    "\n",
    "**示例**：\n",
    "```python\n",
    "# 创建一个迭代器对象\n",
    "my_iterator = iter([1, 2, 3])\n",
    "\n",
    "# 使用 next() 获取下一个值\n",
    "print(next(my_iterator))  # 输出 1\n",
    "print(next(my_iterator))  # 输出 2\n",
    "print(next(my_iterator))  # 输出 3\n",
    "# print(next(my_iterator))  # 抛出 StopIteration 异常\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 区别与联系\n",
    "- **可迭代对象** 是可以被迭代的对象，但不一定能直接调用 `next()` 方法。\n",
    "- **迭代器对象** 是专门用于迭代的对象，可以直接调用 `next()` 方法。\n",
    "- **关系**：每个可迭代对象都可以通过 `iter()` 函数转换为迭代器对象。迭代器对象本身也是一个可迭代对象，因为它实现了 `__iter__()` 方法并返回自身。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 特殊情况：`if iter(data) is data`\n",
    "- **解释**：这行代码检查 `data` 是否既是可迭代对象又是迭代器对象，并且 `iter(data)` 返回的就是 `data` 本身。\n",
    "- **应用场景**：通常用于判断 `data` 是否已经是迭代器对象，而不是普通的可迭代对象。例如，在某些情况下，你可能需要传入可迭代对象作为参数，避免重复创建迭代器对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Extras & Tensorboard logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 extras 的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        # ...\n",
    "        # allocate dictionary to store metrics\n",
    "        self.extras = {}\n",
    "        # ...\n",
    "    def reset(self, seed: int | None = None, options: dict[str, Any] | None = None) -> tuple[VecEnvObs, dict]:\n",
    "        # ...\n",
    "        # reset state of scene\n",
    "        indices = torch.arange(self.num_envs, dtype=torch.int64, device=self.device)\n",
    "        self._reset_idx(indices)  # 传入indices有点奇怪，不知道哪里会使用这个reset函数，后面发现被RslRlVecEnvWrapper()重写了\n",
    "        # ...\n",
    "        # return observations\n",
    "        return self._get_observations(), self.extras\n",
    "    def step(self, action: torch.Tensor) -> VecEnvStepReturn:\n",
    "        \"\"\"Returns:\n",
    "            A tuple containing the observations, rewards, resets (terminated and truncated) and extras.\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        # return observations, rewards, resets and extras\n",
    "        return self.obs_buf, self.reward_buf, self.reset_terminated, self.reset_time_outs, self.extras\n",
    "    # ...\n",
    "    def _reset_idx(self, env_ids: Sequence[int]):\n",
    "        \"\"\"Reset environments based on specified indices.\n",
    "        \"\"\"\n",
    "        self.scene.reset(env_ids)\n",
    "        # ...\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 为extras添加自定义的信息\n",
    "```python\n",
    "self.extras[\"log\"] = dict()\n",
    "self.extras[\"log\"].update(your_log_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".e.g\n",
    "```python\n",
    "class AnymalCEnv(DirectRLEnv):\n",
    "    cfg: AnymalCFlatEnvCfg | AnymalCRoughEnvCfg\n",
    "\n",
    "    def __init__(self, cfg: AnymalCFlatEnvCfg | AnymalCRoughEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        super().__init__(cfg, render_mode, **kwargs)\n",
    "        # ...\n",
    "        # Logging\n",
    "        self._episode_sums = {\n",
    "            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)\n",
    "            for key in [\n",
    "                \"track_lin_vel_xy_exp\",\n",
    "                \"track_ang_vel_z_exp\",\n",
    "                \"lin_vel_z_l2\",\n",
    "                \"ang_vel_xy_l2\",\n",
    "                \"dof_torques_l2\",\n",
    "                \"dof_acc_l2\",\n",
    "                \"action_rate_l2\",\n",
    "                \"feet_air_time\",\n",
    "                \"undesired_contacts\",\n",
    "                \"flat_orientation_l2\",\n",
    "            ]\n",
    "        }\n",
    "        # ...\n",
    "    # ...\n",
    "    def _get_rewards(self) -> torch.Tensor:\n",
    "        # ...\n",
    "        rewards = {\n",
    "            \"track_lin_vel_xy_exp\": lin_vel_error_mapped * self.cfg.lin_vel_reward_scale * self.step_dt,\n",
    "            \"track_ang_vel_z_exp\": yaw_rate_error_mapped * self.cfg.yaw_rate_reward_scale * self.step_dt,\n",
    "            \"lin_vel_z_l2\": z_vel_error * self.cfg.z_vel_reward_scale * self.step_dt,\n",
    "            \"ang_vel_xy_l2\": ang_vel_error * self.cfg.ang_vel_reward_scale * self.step_dt,\n",
    "            \"dof_torques_l2\": joint_torques * self.cfg.joint_torque_reward_scale * self.step_dt,\n",
    "            \"dof_acc_l2\": joint_accel * self.cfg.joint_accel_reward_scale * self.step_dt,\n",
    "            \"action_rate_l2\": action_rate * self.cfg.action_rate_reward_scale * self.step_dt,\n",
    "            \"feet_air_time\": air_time * self.cfg.feet_air_time_reward_scale * self.step_dt,\n",
    "            \"undesired_contacts\": contacts * self.cfg.undersired_contact_reward_scale * self.step_dt,\n",
    "            \"flat_orientation_l2\": flat_orientation * self.cfg.flat_orientation_reward_scale * self.step_dt,\n",
    "        }\n",
    "        reward = torch.sum(torch.stack(list(rewards.values())), dim=0)\n",
    "        # Logging\n",
    "        for key, value in rewards.items():\n",
    "            self._episode_sums[key] += value\n",
    "        return reward\n",
    "    # ...\n",
    "    def _reset_idx(self, env_ids: torch.Tensor | None):\n",
    "        # ...\n",
    "        # Logging\n",
    "        extras = dict()\n",
    "        for key in self._episode_sums.keys():\n",
    "            episodic_sum_avg = torch.mean(self._episode_sums[key][env_ids])\n",
    "            extras[\"Episode_Reward/\" + key] = episodic_sum_avg / self.max_episode_length_s\n",
    "            self._episode_sums[key][env_ids] = 0.0\n",
    "        self.extras[\"log\"] = dict()\n",
    "        self.extras[\"log\"].update(extras)\n",
    "        extras = dict()\n",
    "        extras[\"Episode_Termination/base_contact\"] = torch.count_nonzero(self.reset_terminated[env_ids]).item()\n",
    "        extras[\"Episode_Termination/time_out\"] = torch.count_nonzero(self.reset_time_outs[env_ids]).item()\n",
    "        self.extras[\"log\"].update(extras)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 extras 的包装和改造\n",
    "`extras[\"log\"]` 和 `extras[\"observations\"]`、`extras[\"time_outs\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ~/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/utils/wrappers/rsl_rl/vecenv_wrapper.py\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    # ...\n",
    "    def reset(self) -> tuple[torch.Tensor, dict]:  # noqa: D102  # 忽略缺少公共方法文档字符串的警告\n",
    "        # reset the environment\n",
    "        obs_dict, _ = self.env.reset()  # 前面定义的extra被丢弃了\n",
    "        # return observations\n",
    "        return obs_dict[\"policy\"], {\"observations\": obs_dict}\n",
    "    \n",
    "    def step(self, actions: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "        # record step information\n",
    "        obs_dict, rew, terminated, truncated, extras = self.env.step(actions)\n",
    "        # compute dones for compatibility with RSL-RL\n",
    "        dones = (terminated | truncated).to(dtype=torch.long)\n",
    "        # move extra observations to the extras dict\n",
    "        obs = obs_dict[\"policy\"]\n",
    "        extras[\"observations\"] = obs_dict\n",
    "        # move time out information to the extras dict\n",
    "        # this is only needed for infinite horizon tasks\n",
    "        if not self.unwrapped.cfg.is_finite_horizon:\n",
    "            extras[\"time_outs\"] = truncated\n",
    "        # return the step information\n",
    "        return obs, rew, dones, extras\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 in OnPolicyRunner\n",
    "`ep_infos = []`，每一个step都有 `ep_infos.append(infos[\"log\"])`\n",
    "\n",
    "但是一轮iteration（16steps .e.g）才会`self.log(locals())`，随后`ep_infos.clear()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class OnPolicyRunner:\n",
    "    #...\n",
    "    def log(self, locs: dict, width: int = 80, pad: int = 35):\n",
    "        self.tot_timesteps += self.num_steps_per_env * self.env.num_envs\n",
    "        self.tot_time += locs[\"collection_time\"] + locs[\"learn_time\"]\n",
    "        iteration_time = locs[\"collection_time\"] + locs[\"learn_time\"]\n",
    "\n",
    "        ep_string = \"\"\n",
    "        if locs[\"ep_infos\"]:\n",
    "            for key in locs[\"ep_infos\"][0]:\n",
    "                infotensor = torch.tensor([], device=self.device)  # 对于每个键，初始化一个空的张量infotensor，用于存储该键对应的值\n",
    "                for ep_info in locs[\"ep_infos\"]:  # 遍历每个step len = num_steps_per_env (16 .e.g)\n",
    "                    # handle scalar and zero dimensional tensor infos\n",
    "                    if key not in ep_info:\n",
    "                        continue  # 如果当前step信息中不存在该键，则跳过\n",
    "                    if not isinstance(ep_info[key], torch.Tensor):\n",
    "                        ep_info[key] = torch.Tensor([ep_info[key]])  # 如果该键对应的值不是张量，则将其转换为张量\n",
    "                    if len(ep_info[key].shape) == 0:\n",
    "                        ep_info[key] = ep_info[key].unsqueeze(0)  # 如果该键对应的是零维张量（标量），则升为一维张量，确保后续的cat能够正常工作\n",
    "                    infotensor = torch.cat((infotensor, ep_info[key].to(self.device)))  # 将该键每个step对应的值与infotensor进行拼接\n",
    "                value = torch.mean(infotensor)  # 计算该键本轮所有step的平均值\n",
    "                # log to logger and terminal\n",
    "                if \"/\" in key:\n",
    "                    self.writer.add_scalar(key, value, locs[\"it\"])\n",
    "                    ep_string += f\"\"\"{f'{key}:':>{pad}} {value:.4f}\\n\"\"\"\n",
    "                else:\n",
    "                    self.writer.add_scalar(\"Episode/\" + key, value, locs[\"it\"])\n",
    "                    ep_string += f\"\"\"{f'Mean episode {key}:':>{pad}} {value:.4f}\\n\"\"\"\n",
    "        # ...\n",
    "        # 一些默认统计信息的计算和输出\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ep_infos = []\n",
    "if ep_infos:\n",
    "    print(\"ep_infos = true\")\n",
    "else:\n",
    "    print(ep_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 零维张量（0-dimensional tensor）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中零维张量也称为标量张量Scalar tensor。零维张量没有形状（shape），通常用于表示单个数值。\n",
    "\n",
    "**零维张量的特点：**\n",
    "- **形状**：`torch.Size([])`，表示没有维度。\n",
    "- **大小**：包含一个单一的值。\n",
    "- **操作**：可以像普通标量一样进行数学运算，并且可以通过 `.item()` 方法获取其内部的 Python 标量值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar tensor: 3.140000104904175\n",
      "Shape: torch.Size([])\n",
      "Value: 3.140000104904175\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个零维张量\n",
    "scalar_tensor = torch.tensor(3.14)\n",
    "print(f\"Scalar tensor: {scalar_tensor}\")\n",
    "print(f\"Shape: {scalar_tensor.shape}\")  # 输出: Shape: torch.Size([])\n",
    "print(f\"Value: {scalar_tensor.item()}\")  # 输出: Value: 3.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述代码中\n",
    "```python\n",
    "if len(ep_info[key].shape) == 0:\n",
    "    ep_info[key] = ep_info[key].unsqueeze(0)\n",
    "```\n",
    "检查 `ep_info[key]` 是否为零维张量，如果是，则使用 `unsqueeze(0)` 方法为其增加一个维度，使其变为一维张量。这一步骤是为了确保后续的拼接操作（如 `torch.cat`）能够正常工作，因为 `torch.cat` 需要所有输入张量具有相同的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 嵌套的格式化字符串\n",
    "`ep_string += f\"\"\"{f'{key}:':>{pad}} {value:.4f}\\n\"\"\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 内层的 `f'...':>{pad}`，用于将变量插入到字符串中\n",
    "- 外层的 `f\"\"\"...\"\"\"`，用于进一步格式化字符串\n",
    "  - `:>{pad}` 右对齐并填充空白字符，使整个字符串的宽度达到pad指定的长度，默认右对齐，即`:{pad}`\n",
    "  - 左对齐填充*，则`:*<{pad}`\n",
    "  - `{value:.4f}` 将 value 变量格式化为浮点数，保留四位小数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
