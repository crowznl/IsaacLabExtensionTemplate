{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Register\n",
    "the env_cfg and agent_cfg is how to convey in this framework\n",
    "\n",
    "- 首先要有个概念：我们的env_cfg继承了DirectRLEnvCfg，agent_cfg继承了RslRlOnPolicyRunnerCfg，也就继承了一些方法（函数）和一些默认属性property。当然也有些方法只有声明，没有实现implement，例如the '_get_observations' method就是在我们的env_cfg中具体实现的。\n",
    "\n",
    "- 然后，由env_cfg创建的env又通过RslRlVecEnvWrapper()进行了wrap\n",
    "- 最后传入了OnPolicyRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts/rsl_rl/train.py\n",
    "```python\n",
    "@hydra_task_config(args_cli.task, \"rsl_rl_cfg_entry_point\")\n",
    "def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\n",
    "    # ...\n",
    "    # create isaac environment\n",
    "    env = gym.make(args_cli.task, cfg=env_cfg, render_mode=\"rgb_array\" if args_cli.video else None)\n",
    "    # ...\n",
    "    # wrap around environment for rsl-rl\n",
    "    env = RslRlVecEnvWrapper(env)\n",
    "    # create runner from rsl-rl\n",
    "    runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)\n",
    "    # ...\n",
    "    # run training\n",
    "    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)\n",
    "    # close the simulator\n",
    "    env.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓decorator 负责解析并提供 env_cfg 和 agent_cfg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def hydra_task_config(task_name: str, agent_cfg_entry_point: str) -> Callable:\n",
    "    #...\n",
    "    env_cfg, agent_cfg = register_task_to_hydra(task_name, agent_cfg_entry_point)\n",
    "    #...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def register_task_to_hydra(\n",
    "    task_name: str, agent_cfg_entry_point: str\n",
    ") -> tuple[ManagerBasedRLEnvCfg | DirectRLEnvCfg, dict]：\n",
    "    # ...\n",
    "    env_cfg = load_cfg_from_registry(task_name, \"env_cfg_entry_point\")\n",
    "    agent_cfg = load_cfg_from_registry(task_name, agent_cfg_entry_point)\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def load_cfg_from_registry(task_name: str, entry_point_key: str) -> dict | object:\n",
    "    \"\"\"It supports both YAML and Python configuration files.\n",
    "    If the entry point is a YAML file, it is parsed into a dictionary.\n",
    "    If the entry point is a Python class, it is instantiated and returned.\"\"\"\n",
    "    # obtain the configuration entry point\n",
    "    cfg_entry_point = gym.spec(task_name).kwargs.get(entry_point_key)\n",
    "    # 如果 cfg_entry_point 是一个以 .yaml 结尾的字符串\n",
    "    # 如果 cfg_entry_point 是可调用的（例如是一个函数或类）\n",
    "    # 如果 cfg_entry_point 是字符串（格式为 \"module_name:attr_name\"）\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".e.g例如\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from . import agents\n",
    "from .zbot6b_env_v0 import ZbotBEnv, ZbotBEnvCfg\n",
    "##\n",
    "# Register Gym environments.\n",
    "##\n",
    "gym.register(\n",
    "    id=\"Zbot-6b-walking-v0\",\n",
    "    entry_point=\"Zbot.tasks.moving.zbot6b_direct:ZbotBEnv\",\n",
    "    disable_env_checker=True,\n",
    "    kwargs={\n",
    "        \"env_cfg_entry_point\": ZbotBEnvCfg, \n",
    "        \"rsl_rl_cfg_entry_point\": f\"{agents.__name__}.rsl_rl_ppo_cfg:ZbotSBFlatPPORunnerCfg\",\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scripts/rsl_rl/train.py\n",
    "```python\n",
    "def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):\n",
    "    # ...\n",
    "    # specify directory for logging experiments\n",
    "    log_root_path = os.path.join(\"logs\", \"rsl_rl\", agent_cfg.experiment_name)\n",
    "    log_root_path = os.path.abspath(log_root_path)\n",
    "    print(f\"[INFO] Logging experiment in directory: {log_root_path}\")\n",
    "    # specify directory for logging runs: {time-stamp}_{run_name}\n",
    "    log_dir = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    if agent_cfg.run_name:\n",
    "        log_dir += f\"_{agent_cfg.run_name}\"\n",
    "    log_dir = os.path.join(log_root_path, log_dir)\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.OnPolicyRunner\n",
    "runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class RslRlOnPolicyRunnerCfg:\n",
    "    \"\"\"Configuration of the runner for on-policy algorithms.\"\"\"\n",
    "\n",
    "    seed: int = 42\n",
    "    \"\"\"The seed for the experiment. Default is 42.\"\"\"\n",
    "\n",
    "    device: str = \"cuda:0\"\n",
    "    \"\"\"The device for the rl-agent. Default is cuda:0.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以 agent_cfg.device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "line 29、93 `obs, extras = self.env.get_observations()`其中get_observations()这个attribute\n",
    "\n",
    "是由于train.py中`env = RslRlVecEnvWrapper(env)`进行了wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ~/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/utils/wrappers/rsl_rl/vecenv_wrapper.py\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    # ...\n",
    "    def get_observations(self) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"Returns the current observations of the environment.\"\"\"\n",
    "        if hasattr(self.unwrapped, \"observation_manager\"):\n",
    "            obs_dict = self.unwrapped.observation_manager.compute()\n",
    "        else:\n",
    "            obs_dict = self.unwrapped._get_observations()\n",
    "        return obs_dict[\"policy\"], {\"observations\": obs_dict}\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".e.g例如\n",
    "```python\n",
    "class ZbotBEnv(DirectRLEnv):\n",
    "    cfg: ZbotBEnvCfg\n",
    "    def __init__(self, cfg: ZbotBEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        super().__init__(cfg, render_mode, **kwargs)\n",
    "        # ...\n",
    "    def _get_observations(self) -> dict:\n",
    "        obs = torch.cat(\n",
    "            (\n",
    "                self.body_quat[:,0].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self.body_quat[:,3].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self.body_quat[:,6].reshape(self.scene.cfg.num_envs, -1),\n",
    "                self._commands,\n",
    "                self.joint_vel,\n",
    "                self.joint_pos,\n",
    "                # 4*(3)+3+6+6\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        observations = {\"policy\": obs}\n",
    "        return observations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 init_at_random_ep_len\n",
    "line 89 `init_at_random_ep_len` 在train.py中传入:\n",
    "\n",
    "`runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)`\n",
    "```python\n",
    "    episode_length_buf: torch.Tensor  # current episode duration\n",
    "    \"\"\"Buffer for current episode lengths.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        # ...\n",
    "        self.episode_length_buf = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class ZbotBEnv(DirectRLEnv):\n",
    "    # ...\n",
    "    def _reset_idx(self, env_ids: torch.Tensor | None):\n",
    "        if env_ids is None or len(env_ids) == self.num_envs:\n",
    "            env_ids = self.zbots._ALL_INDICES\n",
    "        self.zbots.reset(env_ids)\n",
    "        super()._reset_idx(env_ids)\n",
    "        if len(env_ids) == self.num_envs:\n",
    "            # Spread out the resets to avoid spikes in training when many environments reset at a similar time\n",
    "            self.episode_length_buf[:] = torch.randint_like(\n",
    "                self.episode_length_buf, high=int(self.max_episode_length)\n",
    "            )\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class OnPolicyRunner:\n",
    "    # ...\n",
    "    def learn(self, num_learning_iterations: int, init_at_random_ep_len: bool = False):\n",
    "        # ...\n",
    "        if init_at_random_ep_len:\n",
    "            self.env.episode_length_buf = torch.randint_like(\n",
    "                self.env.episode_length_buf, high=int(self.env.max_episode_length)\n",
    "            )\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "line109 `with torch.inference_mode()` \n",
    "\n",
    "在 rollout 阶段，模型生成动作并与环境交互，目的是收集数据以供后续训练使用。此时禁用梯度计算，可以优化性能并确保数据收集过程的可靠性。\n",
    "\n",
    "其中：\n",
    "- `self.alg.act(obs, critic_obs)` 生成动作并与环境交互。\n",
    "- `self.env.step(actions)` 执行动作并返回新的观测、奖励、完成标志和其他信息。\n",
    "- `self.obs_normalizer(obs)` 和 `self.critic_obs_normalizer(infos[\"observations\"][\"critic\"])` 对观测进行归一化处理。\n",
    "- `self.alg.process_env_step(rewards, dones, infos)` 处理环境反馈，准备后续的训练数据。\n",
    "\n",
    "训练数据的收集真正靠的是`self.alg.process_env_step(rewards, dones, infos)`传入，\n",
    "\n",
    "而外部的这些变量更多是用于tensorboard和terminal的展示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5\n",
    "line 112 `obs, rewards, dones, infos = self.env.step(actions)`使用四个参数接收返回值，也是因为`RslRlVecEnvWrapper()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原本有5个返回值\n",
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def step(self, action: torch.Tensor) -> VecEnvStepReturn:\n",
    "        \"\"\"Returns:\n",
    "            A tuple containing the observations, rewards, resets (terminated and truncated) and extras.\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        # return observations, rewards, resets and extras\n",
    "        return self.obs_buf, self.reward_buf, self.reset_terminated, self.reset_time_outs, self.extras\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    #...\n",
    "    def step(self, actions: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "        # record step information\n",
    "        obs_dict, rew, terminated, truncated, extras = self.env.step(actions)\n",
    "        # compute dones for compatibility with RSL-RL\n",
    "        dones = (terminated | truncated).to(dtype=torch.long)\n",
    "        # move extra observations to the extras dict\n",
    "        obs = obs_dict[\"policy\"]\n",
    "        extras[\"observations\"] = obs_dict\n",
    "        # move time out information to the extras dict\n",
    "        # this is only needed for infinite horizon tasks\n",
    "        if not self.unwrapped.cfg.is_finite_horizon:\n",
    "            extras[\"time_outs\"] = truncated\n",
    "\n",
    "        # return the step information\n",
    "        return obs, rew, dones, extras\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6\n",
    "```python\n",
    "rewbuffer.extend(cur_reward_sum[new_ids][:, 0].cpu().numpy().tolist())\n",
    "lenbuffer.extend(cur_episode_length[new_ids][:, 0].cpu().numpy().tolist())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 cur_reward_sum[new_ids][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nonzero(as_tuple=False)\n",
    "`new_ids = (dones > 0).nonzero(as_tuple=False)`，`as_tuple=False`表示返回结果是一个二维张量，而不是一个元组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一维张量，nonzero 返回的张量的形状是 (N, 1)，其中 N 是非零元素的数量。\n",
    "```python\n",
    "print(cur_reward_sum.shape)  # torch.Size([64])\n",
    "print(new_ids.shape)  # torch.Size([2, 1])\n",
    "print(new_ids)  # tensor([[1], [3]], device='cuda:0')\n",
    "print(cur_reward_sum[new_ids].shape)  # torch.Size([2, 1])  # 升维了\n",
    "```\n",
    "通过高级索引，改变了shape，从一维变成了二维，因此`cur_reward_sum[new_ids][:, 0]`才没抛出异常。（不能对一维进行[:, 0]）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级索引（Advanced Indexing）\n",
    "\n",
    "高级索引允许你使用整数数组来选择张量中的元素。高级索引的行为与基本切片（basic slicing）不同，它会返回一个与**索引数组形状相同的新张量**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "tensor([[-2.1058],\n",
      "        [ 0.1667],\n",
      "        [ 1.7387],\n",
      "        [-0.5361]], device='cuda:0')\n",
      "torch.Size([4, 1])\n",
      "tensor([[-2.1058,  0.1667],\n",
      "        [ 1.7387, -0.5361],\n",
      "        [ 1.7387, -0.5361],\n",
      "        [ 1.7387, -0.5361]], device='cuda:0')\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 初始化 A 和 new_ids\n",
    "A = torch.randn(64, dtype=torch.float, device='cuda')\n",
    "print(A.shape)  # 输出: torch.Size([64])\n",
    "new_ids = torch.tensor([[1], [3], [6], [8]], dtype=torch.long, device='cuda')\n",
    "# 计算 A[new_ids]\n",
    "result = A[new_ids]\n",
    "print(result)\n",
    "print(result.shape)  # 输出: torch.Size([4, 1])\n",
    "\n",
    "new_ids_2 = torch.tensor([[1, 3], [6, 8], [6, 8], [6, 8]], dtype=torch.long, device='cuda')\n",
    "result_2 = A[new_ids_2]\n",
    "print(result_2)\n",
    "print(result_2.shape)  # 输出: torch.Size([4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 deque\n",
    "虽然rewbuffer每个step都在更新(除非某一步所有env都没有reset)，但每个iteration(16steps .e.g)才计算rewbuffer平均值并更log一次\n",
    "\n",
    "同时rewbuffer的maxlen=100，前期一个step的reset的env数量多，可能都不够装，只能记录后100个reset的env的值\n",
    "\n",
    "也就是说log时，可能只用到了一轮iteration中最后一个step的后100个reset_env的值，来计算平均值\n",
    "\n",
    "当然，通过前面提到的`init_at_random_ep_len`机制，reset的env分散了，不会总是固定的那100个env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], maxlen=100)\n",
      "1\n",
      "deque([0, 1, 2], maxlen=3)\n",
      "deque([2, 3, 4], maxlen=3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import statistics\n",
    "\n",
    "rewbuffer = deque(maxlen=100)\n",
    "rewbuffer.extend([1, 1, 1, 1])\n",
    "rewbuffer.extend([1, 1, 1, 1, 1])\n",
    "rewbuffer.extend([1, 1, 1])\n",
    "print(rewbuffer)\n",
    "print(statistics.mean(rewbuffer))\n",
    "\n",
    "small_buffer = deque(maxlen=3)\n",
    "small_buffer.extend([-1, 0, 1, 2])\n",
    "print(small_buffer)\n",
    "small_buffer.extend([3, 4])\n",
    "print(small_buffer)\n",
    "print(statistics.mean(small_buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7\n",
    "line154 `self.log(locals())`， log的详解见#3\n",
    "\n",
    "↓\n",
    "\n",
    "line205 `self.writer.add_scalar(\"Train/mean_reward\", statistics.mean(locs[\"rewbuffer\"]), locs[\"it\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迭代器对象和可迭代对象的区别\n",
    "\n",
    "在 Python 中，**迭代器对象** 和 **可迭代对象** 是两个不同的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 可迭代对象（Iterable）\n",
    "- **定义**：可迭代对象是实现了 `__iter__()` 方法的对象。这个方法返回一个迭代器对象。\n",
    "- **特点**：\n",
    "  - 可以通过 `for` 循环遍历。\n",
    "  - 可以传递给内置函数 `iter()` 来获取迭代器对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **常见类型**：\n",
    "  - 列表 (`list`)\n",
    "  - 元组 (`tuple`)\n",
    "  - 字符串 (`str`)\n",
    "  - 字典 (`dict`)\n",
    "  - 集合 (`set`)\n",
    "  - 文件对象\n",
    "  - 自定义的可迭代类\n",
    "\n",
    "**示例**：\n",
    "```python\n",
    "# 列表是一个可迭代对象\n",
    "my_list = [1, 2, 3]\n",
    "for item in my_list:\n",
    "    print(item)\n",
    "\n",
    "# 使用 iter() 获取迭代器对象\n",
    "iterator = iter(my_list)  # 大多数情况下，iter(my_list) 等价于 my_list.__iter__()\n",
    "print(next(iterator))  # 输出 1\n",
    "print(next(iterator))  # 输出 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 迭代器对象（Iterator）\n",
    "- **定义**：迭代器对象是实现了 `__next__()` 方法的对象。每次调用 `next()` 函数时，它会返回下一个值，直到没有更多元素为止，此时会抛出 `StopIteration` 异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **特点**：\n",
    "  - 必须实现 `__iter__()` 方法，返回自身（即 `self`）。\n",
    "  - 每次调用 `next()` 返回序列中的下一个值。\n",
    "  - 一旦遍历结束，再次调用 `next()` 会抛出 `StopIteration` 异常。\n",
    "- **常见类型**：\n",
    "  - 由 `iter()` 函数从可迭代对象生成的对象。\n",
    "  - 生成器（Generator）\n",
    "\n",
    "**示例**：\n",
    "```python\n",
    "# 创建一个迭代器对象\n",
    "my_iterator = iter([1, 2, 3])\n",
    "\n",
    "# 使用 next() 获取下一个值\n",
    "print(next(my_iterator))  # 输出 1\n",
    "print(next(my_iterator))  # 输出 2\n",
    "print(next(my_iterator))  # 输出 3\n",
    "# print(next(my_iterator))  # 抛出 StopIteration 异常\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 区别与联系\n",
    "- **可迭代对象** 是可以被迭代的对象，但不一定能直接调用 `next()` 方法。\n",
    "- **迭代器对象** 是专门用于迭代的对象，可以直接调用 `next()` 方法。\n",
    "- **关系**：每个可迭代对象都可以通过 `iter()` 函数转换为迭代器对象。迭代器对象本身也是一个可迭代对象，因为它实现了 `__iter__()` 方法并返回自身。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 特殊情况：`if iter(data) is data`\n",
    "- **解释**：这行代码检查 `data` 是否既是可迭代对象又是迭代器对象，并且 `iter(data)` 返回的就是 `data` 本身。\n",
    "- **应用场景**：通常用于判断 `data` 是否已经是迭代器对象，而不是普通的可迭代对象。例如，在某些情况下，你可能需要传入可迭代对象作为参数，避免重复创建迭代器对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Extras & Tensorboard logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class our_env_cfg(DirectRLEnvCfg)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DirectRLEnv(gym.Env):\n",
    "    # ...\n",
    "    def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):\n",
    "        # ...\n",
    "        # allocate dictionary to store metrics\n",
    "        self.extras = {}\n",
    "        # ...\n",
    "    def reset(self, seed: int | None = None, options: dict[str, Any] | None = None) -> tuple[VecEnvObs, dict]:\n",
    "        # ...\n",
    "        # reset state of scene\n",
    "        indices = torch.arange(self.num_envs, dtype=torch.int64, device=self.device)\n",
    "        self._reset_idx(indices)  # 传入indices有点奇怪，不知道哪里会使用这个reset函数，后面发现被RslRlVecEnvWrapper()重写了\n",
    "        # ...\n",
    "        # return observations\n",
    "        return self._get_observations(), self.extras\n",
    "    def step(self, action: torch.Tensor) -> VecEnvStepReturn:\n",
    "        \"\"\"Returns:\n",
    "            A tuple containing the observations, rewards, resets (terminated and truncated) and extras.\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        # return observations, rewards, resets and extras\n",
    "        return self.obs_buf, self.reward_buf, self.reset_terminated, self.reset_time_outs, self.extras\n",
    "    # ...\n",
    "    def _reset_idx(self, env_ids: Sequence[int]):\n",
    "        \"\"\"Reset environments based on specified indices.\n",
    "        \"\"\"\n",
    "        self.scene.reset(env_ids)\n",
    "        # ...\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ~/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/utils/wrappers/rsl_rl/vecenv_wrapper.py\n",
    "class RslRlVecEnvWrapper(VecEnv):\n",
    "    # ...\n",
    "    def reset(self) -> tuple[torch.Tensor, dict]:  # noqa: D102  # 忽略缺少公共方法文档字符串的警告\n",
    "        # reset the environment\n",
    "        obs_dict, _ = self.env.reset()  # 前面定义的extra被丢弃了\n",
    "        # return observations\n",
    "        return obs_dict[\"policy\"], {\"observations\": obs_dict}\n",
    "    #...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ep_infos = []\n",
    "if ep_infos:\n",
    "    print(\"ep_infos = true\")\n",
    "else:\n",
    "    print(ep_infos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
